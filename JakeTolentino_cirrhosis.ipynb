{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f8c1ca01-84d7-4e4f-8ccf-aeae36e14e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary libraries\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, cross_validate\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, make_scorer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.metrics import classification_report\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "# Turn off all warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09595bc9-320f-48ec-aeec-757fa619ad9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of missing values:\n",
      "N_Days            0\n",
      "Status            0\n",
      "Drug              0\n",
      "Age               0\n",
      "Sex               0\n",
      "Ascites           0\n",
      "Hepatomegaly      0\n",
      "Spiders           0\n",
      "Edema             0\n",
      "Bilirubin         0\n",
      "Cholesterol      28\n",
      "Albumin           0\n",
      "Copper            2\n",
      "Alk_Phos          0\n",
      "SGOT              0\n",
      "Tryglicerides    30\n",
      "Platelets         4\n",
      "Prothrombin       0\n",
      "Stage             0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "data = pd.read_csv('//Users/jake/ML/cirrhosis.csv')\n",
    "\n",
    "# Drop all the rows where missing values were present in the Drug column and drop the ID column\n",
    "data_cleaned = data.dropna(subset=['Drug']).drop(columns=['ID'])\n",
    "\n",
    "# Show sum of missing values\n",
    "missing_values_sum = data_cleaned.isnull().sum()\n",
    "print(\"Sum of missing values:\")\n",
    "print(missing_values_sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b04b3f8-af56-48ab-9c81-0aa7d6fa2d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Continuous Features: ['N_Days', 'Age', 'Bilirubin', 'Cholesterol', 'Albumin', 'Copper', 'Alk_Phos', 'SGOT', 'Tryglicerides', 'Platelets', 'Prothrombin', 'Stage']\n",
      "Categorical Features: ['Status', 'Drug', 'Sex', 'Ascites', 'Hepatomegaly', 'Spiders', 'Edema']\n",
      "\n",
      "Label distribution in the training set:\n",
      "Status\n",
      "C     0.558233\n",
      "D     0.397590\n",
      "CL    0.044177\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "Is the training set balanced?: True\n"
     ]
    }
   ],
   "source": [
    "# Impute missing values with mean results (only for numeric columns)\n",
    "numeric_cols = data_cleaned.select_dtypes(include=[np.number]).columns\n",
    "data_cleaned[numeric_cols] = data_cleaned[numeric_cols].fillna(data_cleaned[numeric_cols].mean())\n",
    "\n",
    "# Split the data set into training and test set with a ratio of 8:2\n",
    "train_data, test_data = train_test_split(data_cleaned, test_size=0.2, random_state=42)\n",
    "\n",
    "# Based on the training and test data, show the feature types and indicate which features are continuous or categorical\n",
    "continuous_features = train_data.select_dtypes(include=[np.number]).columns.tolist()\n",
    "categorical_features = train_data.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "\n",
    "print(\"\\nContinuous Features:\", continuous_features)\n",
    "print(\"Categorical Features:\", categorical_features)\n",
    "\n",
    "# One-hot encoding for all category attributes\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "encoded_train_data = pd.DataFrame(encoder.fit_transform(train_data[categorical_features]), columns=encoder.get_feature_names_out(categorical_features))\n",
    "encoded_test_data = pd.DataFrame(encoder.transform(test_data[categorical_features]), columns=encoder.get_feature_names_out(categorical_features))\n",
    "\n",
    "train_data_encoded = pd.concat([train_data.reset_index(drop=True), encoded_train_data], axis=1).drop(columns=categorical_features)\n",
    "test_data_encoded = pd.concat([test_data.reset_index(drop=True), encoded_test_data], axis=1).drop(columns=categorical_features)\n",
    "\n",
    "# Show the label distribution based on the training data, is it a balanced training set\n",
    "label_distribution = train_data['Status'].value_counts(normalize=True)\n",
    "print(\"\\nLabel distribution in the training set:\")\n",
    "print(label_distribution)\n",
    "print(\"\\nIs the training set balanced?:\", label_distribution.max() < 0.7)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "536d2fdb-4f9c-47d0-9dcf-51b4c9d9b397",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'Logistic Regression': {'accuracy': 0.6947755102040817,\n",
       "   'precision': 0.6677079359667015,\n",
       "   'recall': 0.6947755102040817,\n",
       "   'f1': 0.6700974732737949},\n",
       "  'Random Forest': {'accuracy': 0.6706938775510204,\n",
       "   'precision': 0.6955220453731732,\n",
       "   'recall': 0.6706938775510204,\n",
       "   'f1': 0.6167548652825409},\n",
       "  'Support Vector Machine': {'accuracy': 0.6706938775510205,\n",
       "   'precision': 0.6613702901879568,\n",
       "   'recall': 0.6706938775510205,\n",
       "   'f1': 0.6359097774942303}},\n",
       " {'Logistic Regression': 'Logistic Regression is a simple and interpretable model, often used as a baseline.',\n",
       "  'Random Forest': 'Random Forest is an ensemble method that reduces variance and avoids overfitting by averaging multiple decision trees.',\n",
       "  'Support Vector Machine': 'SVM is effective in high-dimensional spaces and works well for classification tasks with clear margins of separation.'})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ensure 'Status' column is in the original train_data before encoding\n",
    "y_train = train_data['Status']\n",
    "\n",
    "# Dropping the 'Status' column from the features before encoding\n",
    "X_train = train_data.drop(columns=['Status'])\n",
    "\n",
    "# Perform one-hot encoding on the categorical features\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_train_encoded = pd.DataFrame(encoder.fit_transform(X_train), columns=encoder.get_feature_names_out(X_train.columns))\n",
    "\n",
    "# Create three supervised machine learning models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Validation method and performance metric\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each model and store results\n",
    "results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_results = cross_validate(model, X_train_encoded, y_train, cv=5, scoring=scoring)\n",
    "    results[model_name] = {\n",
    "        'accuracy': cv_results['test_accuracy'].mean(),\n",
    "        'precision': cv_results['test_precision'].mean(),\n",
    "        'recall': cv_results['test_recall'].mean(),\n",
    "        'f1': cv_results['test_f1'].mean()\n",
    "    }\n",
    "\n",
    "# Justification of model choices\n",
    "model_justifications = {\n",
    "    'Logistic Regression': \"Logistic Regression is a simple and interpretable model, often used as a baseline.\",\n",
    "    'Random Forest': \"Random Forest is an ensemble method that reduces variance and avoids overfitting by averaging multiple decision trees.\",\n",
    "    'Support Vector Machine': \"SVM is effective in high-dimensional spaces and works well for classification tasks with clear margins of separation.\"\n",
    "}\n",
    "\n",
    "# Display results and justifications\n",
    "results, model_justifications\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7ef98832-c396-44b3-9c43-ab6a8ecef5ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 10 candidates, totalling 50 fits\n",
      "Fitting 5 folds for each of 108 candidates, totalling 540 fits\n",
      "Fitting 5 folds for each of 32 candidates, totalling 160 fits\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "({'Logistic Regression': {'C': 1, 'penalty': 'l1', 'solver': 'liblinear'},\n",
       "  'Random Forest': {'max_depth': None,\n",
       "   'min_samples_leaf': 2,\n",
       "   'min_samples_split': 5,\n",
       "   'n_estimators': 200},\n",
       "  'Support Vector Machine': {'C': 0.1, 'gamma': 1, 'kernel': 'linear'}},\n",
       " {'Logistic Regression': 0.6930697626822789,\n",
       "  'Random Forest': 0.663759490083859,\n",
       "  'Support Vector Machine': 0.6703101919781055})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the hyper-parameter grids for each model\n",
    "param_grids = {\n",
    "    'Logistic Regression': {\n",
    "        'C': [0.01, 0.1, 1, 10, 100],\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'solver': ['liblinear']  # 'liblinear' supports l1 penalty\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'n_estimators': [100, 200, 300],\n",
    "        'max_depth': [None, 10, 20, 30],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'Support Vector Machine': {\n",
    "        'C': [0.1, 1, 10, 100],\n",
    "        'gamma': [1, 0.1, 0.01, 0.001],\n",
    "        'kernel': ['rbf', 'linear']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Custom scoring method\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform GridSearchCV for each model\n",
    "best_params = {}\n",
    "best_scores = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    grid_search = GridSearchCV(estimator=model, param_grid=param_grids[model_name], scoring='f1_weighted', cv=5, n_jobs=-1, verbose=1)\n",
    "    grid_search.fit(X_train_encoded, y_train)\n",
    "    \n",
    "    best_params[model_name] = grid_search.best_params_\n",
    "    best_scores[model_name] = grid_search.best_score_\n",
    "\n",
    "# Display the best parameters and scores for each model\n",
    "best_params, best_scores\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8947f1d-413e-46ae-9a82-65c6ad104fe2",
   "metadata": {},
   "source": [
    "Hyper-parameter optimization is a crucial step in building effective machine learning models. By carefully selecting and tuning hyper-parameters using methods like GridSearchCV, we can significantly improve model performance, ensure better generalization, and adapt the models to the specific characteristics of the dataset. This process helps to achieve a balance between underfitting and overfitting, ultimately leading to more reliable and accurate predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8d3ac201-9b8e-45c2-afd2-ec3e1c8f5a6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Balanced Dataset Results: {'Logistic Regression': {'accuracy': 0.8825014343086632, 'precision': 0.8904895304524688, 'recall': 0.8825014343086632, 'f1': 0.88127437405917}, 'Random Forest': {'accuracy': 0.8442914515203672, 'precision': 0.8676163040185079, 'recall': 0.8442914515203672, 'f1': 0.8372457408549977}, 'Support Vector Machine': {'accuracy': 0.86815834767642, 'precision': 0.8833621564503791, 'recall': 0.86815834767642, 'f1': 0.8653795611352366}}\n",
      "Imbalanced Dataset Results: {'Logistic Regression': 0.6930697626822789, 'Random Forest': 0.663759490083859, 'Support Vector Machine': 0.6703101919781055}\n",
      "The best model after balancing the dataset is: Logistic Regression\n",
      "Performance metrics: {'accuracy': 0.8825014343086632, 'precision': 0.8904895304524688, 'recall': 0.8825014343086632, 'f1': 0.88127437405917}\n"
     ]
    }
   ],
   "source": [
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train_encoded, y_train)\n",
    "\n",
    "# Retrain the models on the balanced dataset\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(max_iter=1000, random_state=42),\n",
    "    'Random Forest': RandomForestClassifier(random_state=42),\n",
    "    'Support Vector Machine': SVC(random_state=42)\n",
    "}\n",
    "\n",
    "# Custom scoring method\n",
    "scoring = {\n",
    "    'accuracy': make_scorer(accuracy_score),\n",
    "    'precision': make_scorer(precision_score, average='weighted'),\n",
    "    'recall': make_scorer(recall_score, average='weighted'),\n",
    "    'f1': make_scorer(f1_score, average='weighted')\n",
    "}\n",
    "\n",
    "# Perform cross-validation for each model on the balanced dataset and store results\n",
    "balanced_results = {}\n",
    "for model_name, model in models.items():\n",
    "    cv_results = cross_validate(model, X_train_balanced, y_train_balanced, cv=5, scoring=scoring)\n",
    "    balanced_results[model_name] = {\n",
    "        'accuracy': cv_results['test_accuracy'].mean(),\n",
    "        'precision': cv_results['test_precision'].mean(),\n",
    "        'recall': cv_results['test_recall'].mean(),\n",
    "        'f1': cv_results['test_f1'].mean()\n",
    "    }\n",
    "\n",
    "# Compare the results with those from the imbalanced dataset\n",
    "print(\"Balanced Dataset Results:\", balanced_results)\n",
    "print(\"Imbalanced Dataset Results:\", best_scores)  # from the previous step\n",
    "\n",
    "# Model recommendation based on the results\n",
    "# Compare the balanced_results with best_scores and choose the best performing model\n",
    "best_model = max(balanced_results, key=lambda x: balanced_results[x]['f1'])\n",
    "print(f\"The best model after balancing the dataset is: {best_model}\")\n",
    "print(f\"Performance metrics: {balanced_results[best_model]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c477595e-cd54-4ddc-b2d1-223b714ff67e",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (4140188169.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[8], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    Preprocess the data\u001b[0m\n\u001b[0m               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "Preprocess the data\n",
    "encoder = OneHotEncoder(sparse_output=False, drop='first')\n",
    "X_encoded = pd.DataFrame(encoder.fit_transform(train_data.drop(columns=['Status'])), columns=encoder.get_feature_names_out(train_data.drop(columns=['Status']).columns))\n",
    "\n",
    "# Split the dataset into training and test sets (using 80/20 split)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_encoded, train_data['Status'], test_size=0.2, random_state=42)\n",
    "\n",
    "# Apply SMOTE to balance the dataset\n",
    "smote = SMOTE(random_state=42)\n",
    "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
    "\n",
    "# Train the Logistic Regression model on the balanced training data\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Perform predictions on the pre-processed test set\n",
    "y_pred = logistic_model.predict(X_test)\n",
    "\n",
    "# Report the model performance\n",
    "performance_report = classification_report(y_test, y_pred)\n",
    "\n",
    "# Print the performance report\n",
    "print(\"Logistic Regression Model Performance after SMOTE:\")\n",
    "print(performance_report)\n",
    "\n",
    "# If you have previous performance metrics from models without SMOTE, compare them here\n",
    "# For example:\n",
    "# print(\"Random Forest Model Performance (before SMOTE):\")\n",
    "# print(random_forest_performance_report)  # This should be the performance report of the Random Forest before applying SMOTE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a30da3-4fc8-41bc-bfd2-c1fb579331dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a Random Forest model\n",
    "rf_model = RandomForestClassifier(random_state=42)\n",
    "rf_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Extract feature importance from Random Forest\n",
    "feature_importances_rf = rf_model.feature_importances_\n",
    "features_rf = X_train_balanced.columns\n",
    "\n",
    "# Create a DataFrame for Random Forest feature importance\n",
    "importance_rf_df = pd.DataFrame({\n",
    "    'Feature': features_rf,\n",
    "    'Importance': feature_importances_rf\n",
    "}).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Train a Logistic Regression model\n",
    "logistic_model = LogisticRegression(max_iter=1000, random_state=42)\n",
    "logistic_model.fit(X_train_balanced, y_train_balanced)\n",
    "\n",
    "# Extract coefficients from Logistic Regression\n",
    "coefficients_log = logistic_model.coef_[0]\n",
    "features_log = X_train_balanced.columns\n",
    "\n",
    "# Create a DataFrame for Logistic Regression coefficients\n",
    "coefficients_log_df = pd.DataFrame({\n",
    "    'Feature': features_log,\n",
    "    'Coefficient': coefficients_log\n",
    "}).sort_values(by='Coefficient', ascending=False)\n",
    "\n",
    "# Visualize the top features side by side\n",
    "def plot_top_features_side_by_side(importance_df_rf, coefficients_df_log, top_n=10):\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(20, 8))\n",
    "\n",
    "    # Plot for Random Forest\n",
    "    sns.barplot(x='Importance', y='Feature', data=importance_df_rf.head(top_n), ax=axes[0])\n",
    "    axes[0].set_title(f'Top {top_n} Features by Importance (Random Forest)')\n",
    "    axes[0].set_xlabel('Feature Importance')\n",
    "    axes[0].set_ylabel('Feature')\n",
    "\n",
    "    # Plot for Logistic Regression\n",
    "    sns.barplot(x='Coefficient', y='Feature', data=coefficients_df_log.head(top_n), ax=axes[1])\n",
    "    axes[1].set_title(f'Top {top_n} Features by Coefficient (Logistic Regression)')\n",
    "    axes[1].set_xlabel('Coefficient Value')\n",
    "    axes[1].set_ylabel('Feature')\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Plot the top 10 features side by side\n",
    "plot_top_features_side_by_side(importance_rf_df, coefficients_log_df, top_n=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3fd72fd-cf82-48ff-be6d-9f4fcbc4926d",
   "metadata": {},
   "source": [
    "Random Forest: Provides a robust and non-linear approach to feature importance, highlighting features that contribute to complex decision rules and interactions. Logistic Regression: Offers clear and interpretable insights into features with the strongest linear effects on the target variable. Combined Interpretation: By using both models, we can gain a comprehensive understanding of feature importance, capturing both linear and non-linear effects. This dual approach is statistically advantageous as it ensures that no important relationships in the data are overlooked, leading to a more thorough and reliable analysis of the factors influencing the prediction of \"Status.\" When analyzing feature importance across two different models—Random Forest and Logistic Regression—each model provides a different perspective on which features are most influential for predicting the target variable (\"Status\"). Here's a statistical breakdown of the findings and reasons behind them:\n",
    "\n",
    "1. Random Forest Feature Importance:\n",
    "Nature of Random Forest:\n",
    "\n",
    "Random Forest is an ensemble method that constructs multiple decision trees during training. Each tree is built by selecting random subsets of features and data points. The importance of a feature in a Random Forest is typically determined by how much it decreases the weighted impurity (e.g., Gini impurity or entropy) in a tree across all the trees in the forest.\n",
    "Statistical Reasoning:\n",
    "\n",
    "Non-Linearity: Random Forest can capture complex, non-linear interactions between features. Therefore, a feature that may not appear important in a linear model (like Logistic Regression) might be critical in a Random Forest if it plays a crucial role in certain decision paths within the trees.\n",
    "Robustness: Since Random Forest averages the results of multiple trees, it tends to be more robust to noise. Features that consistently reduce impurity across various trees are considered important.\n",
    "Feature Interactions: Random Forests can implicitly capture interactions between features. If two or more features together lead to significant impurity reduction in the trees, they may be given higher importance even if they are not as influential individually.\n",
    "Finding: The top features identified by Random Forest are those that most effectively partition the data into homogeneous subsets across multiple decision trees, indicating their importance in capturing complex relationships in the data.\n",
    "\n",
    "2. Logistic Regression Coefficients:\n",
    "Nature of Logistic Regression:\n",
    "\n",
    "Logistic Regression is a linear model that estimates the probability of a binary outcome based on a linear combination of input features. The model's coefficients represent the log-odds of the outcome for a one-unit increase in the corresponding feature, assuming all other features are held constant.\n",
    "Statistical Reasoning:\n",
    "\n",
    "Linearity: Logistic Regression assumes a linear relationship between each feature and the log-odds of the outcome. Therefore, features with large absolute coefficients are those that have the strongest linear association with the target variable.\n",
    "Additivity: The effect of each feature is additive, meaning the model does not account for interactions between features unless interaction terms are explicitly included.\n",
    "Interpretability: Coefficients in Logistic Regression are easy to interpret. A positive coefficient increases the log-odds of the positive class, while a negative coefficient decreases it. The magnitude of the coefficient indicates the strength of the relationship.\n",
    "Finding: The top features identified by Logistic Regression are those with the strongest linear relationship with the target variable, making them influential in determining the outcome in a linear context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b17b5922-cb15-41dd-a263-375f1b247ff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
